## Step 4: 激活函数实验笔记

### Q1｜ReLU 对数据有什么作用？
**回答：**  
ReLU 会把输入小于 0 的值置为 0，大于 0 的值保持不变。  
在二维点的实验中，它把原本散乱的点“压”到正象限，使投影后的点沿着某个方向呈现更清晰的排列。

---

### Q2｜为什么投影后形成了一条线？
**回答：**  
经过 ReLU 非线性映射后，原本的负坐标被置零，相当于在某些维度上消除了干扰。  
当我们选择一个方向向量 v 投影时，点沿投影方向聚集，呈现出类似“一条直线”的结构。  
这直观展示了激活函数增强线性可分性的效果。

---

### Q3｜这个实验和神经网络有什么联系？
**回答：**  
- ReLU 或其他激活函数为模型提供非线性能力  
- 可以把原本线性不可分的数据映射到可分空间  
- 投影方向 v 类似神经网络的权重，决定了信息加权方向  
- 本质是深度学习隐藏层 + 激活函数原理的直观实验

---

### 总结
> 激活函数能够让数据在投影空间更可分，这是神经网络捕捉复杂非线性关系的核心机制。
