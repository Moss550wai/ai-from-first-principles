
# Step9 — Gradient Descent & Local Minima

## Concept

在理想情况下，我们希望通过梯度下降找到 **全局最小值**。
但真实机器学习中的 Loss 函数通常是 **非凸函数**，其地形非常复杂，包含：

* 多个局部最小值（local minima）
* 平坦区域（plateau）
* 鞍点（saddle point）

因此，优化问题本质上是：

在崎岖的高维空间中寻找一个“足够低”的谷底。

---

## Why Linear Regression Is Easy

线性回归的损失函数是 **凸函数（convex function）**。

```
L(w) = (1/n) Σ (wx + b − y)^2
```

凸函数的关键性质：

* 只有一个最低点
* 梯度下降一定能找到全局最优

因此线性回归训练非常稳定。

---

## Why Neural Networks Are Hard

神经网络的 Loss 是：

```
L(W1, W2, ..., Wn)
```

由于存在：

* 多层结构
* 非线性激活函数
* 高维参数空间

Loss 变为 **非凸优化问题**。

这意味着：

* 存在大量局部最优
* 训练路径依赖初始化
* 不同起点可能得到不同结果

---

## Local Minimum

局部最优定义：

在某个小区域内最低，但不是全局最低。

特点：

```
梯度 = 0
但仍存在更低的点
```

梯度下降会停在这里。

---

## Saddle Point (更重要)

在高维空间中，更常见的是 **鞍点**。

鞍点特征：

* 某些方向向上
* 某些方向向下
* 梯度 ≈ 0

模型会误以为已经收敛，训练速度极慢。

深度学习的大量优化困难来自鞍点，而不是局部最小。

---

## Experiment Summary

本实验使用函数：

```
f(w) = w^2 + 3 sin(5w)
```

该函数具有多个局部最小值。

实验方法：

* 从不同初始点启动梯度下降
* 观察最终收敛位置

实验现象：

* 不同初始点 → 落入不同谷底
* 证明梯度下降具有路径依赖

---

## Why Deep Learning Still Works

现代深度学习通过优化技巧缓解该问题：

| 技术       | 作用                |
| -------- | ----------------- |
| SGD 随机性  | 帮助跳出局部最优          |
| Momentum | 穿越平坦区域            |
| Adam     | 自适应学习率            |
| 大规模数据    | 平滑 loss landscape |

关键理解：

训练目标不是找到完美最低点，而是找到 **足够好的解**。

---

## Key Takeaways

* 线性回归 → 凸优化 → 全局最优
* 神经网络 → 非凸优化 → 多局部最优
* 梯度下降具有路径依赖
* 优化算法是深度学习成功的关键因素之一


